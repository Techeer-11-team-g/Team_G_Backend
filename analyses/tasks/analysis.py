"""
Image Analysis Tasks - ì´ë¯¸ì§€ ë¶„ì„ íŒŒì´í”„ë¼ì¸.

Pipeline:
1. Receive image analysis request
2. Google Vision API for object detection (bbox)
3. Crop detected items with padding
4. Upload cropped images to GCS
5. Extract attributes with Claude Haiku (ë³‘ë ¬ ì²˜ë¦¬)
6. Generate embeddings using FashionCLIP (ë³‘ë ¬ ì²˜ë¦¬)
7. Hybrid search in OpenSearch (k-NN + keyword) (ë³‘ë ¬ ì²˜ë¦¬)
8. Claude Haiku reranking (ë³‘ë ¬ ì²˜ë¦¬)
9. Save results to MySQL
10. Update status in Redis

ë¹„ë™ê¸° ì²˜ë¦¬:
- RabbitMQ: ë©”ì‹œì§€ ë¸Œë¡œì»¤ë¡œ íƒœìŠ¤í¬ íì‰
- Celery Group: ì—¬ëŸ¬ ê°ì²´ë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬
- Redis: ì§„í–‰ ìƒíƒœ ë° ê²°ê³¼ ìºì‹±
"""

import io
import base64
import logging
import uuid
from datetime import datetime
from typing import Optional

from celery import shared_task, chord
from django.conf import settings
from PIL import Image
from google.cloud import storage

from services.vision_service import get_vision_service, DetectedItem
from services.embedding_service import get_embedding_service
from services.opensearch_client import OpenSearchService
from services.redis_service import get_redis_service
from services.metrics import (
    ANALYSIS_TOTAL,
    ANALYSIS_DURATION,
    ANALYSIS_IN_PROGRESS,
    ANALYSES_COMPLETED_TOTAL,
    PRODUCT_MATCHES_TOTAL,
    push_metrics,
)

logger = logging.getLogger(__name__)


def _upload_to_gcs(image_bytes: bytes, analysis_id: str, item_index: int, category: str) -> Optional[str]:
    """
    Upload cropped image to GCS.

    Args:
        image_bytes: Cropped image bytes
        analysis_id: Analysis job ID
        item_index: Item index
        category: Item category

    Returns:
        GCS public URL or None if upload fails
    """
    try:
        bucket_name = settings.GCS_BUCKET_NAME
        credentials_file = settings.GCS_CREDENTIALS_FILE

        if not bucket_name or not credentials_file:
            logger.warning("GCS not configured, skipping upload")
            return None

        # Create storage client
        client = storage.Client.from_service_account_json(credentials_file)
        bucket = client.bucket(bucket_name)

        # Generate unique filename
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"cropped/{analysis_id}/{timestamp}_{item_index}_{category}.jpg"

        # Upload
        blob = bucket.blob(filename)
        blob.upload_from_string(image_bytes, content_type='image/jpeg')

        # Return public URL
        gcs_url = f"https://storage.googleapis.com/{bucket_name}/{filename}"
        logger.info(f"Uploaded cropped image to GCS: {gcs_url}")

        return gcs_url

    except Exception as e:
        logger.error(f"Failed to upload to GCS: {e}")
        return None


@shared_task(bind=True, max_retries=3, default_retry_delay=60)
def process_image_analysis(
    self,
    analysis_id: str,
    image_url: str,
    user_id: Optional[int] = None,
):
    """
    ì´ë¯¸ì§€ ë¶„ì„ ë©”ì¸ íƒœìŠ¤í¬.

    Vision APIë¡œ ê°ì²´ ê²€ì¶œ í›„, Celery Groupì„ ì‚¬ìš©í•˜ì—¬
    ê° ê°ì²´ë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤ (ì„ë² ë”© ìƒì„± + ê²€ìƒ‰ + ë¦¬ë­í‚¹).

    Args:
        analysis_id: Analysis job ID
        image_url: GCS URL of the uploaded image
        user_id: Optional user ID

    Returns:
        Analysis result dict
    """
    redis_service = get_redis_service()
    ANALYSIS_IN_PROGRESS.inc()

    try:
        # Update status to RUNNING
        redis_service.update_analysis_running(analysis_id, progress=0)
        logger.info(f"Starting analysis {analysis_id}")

        # Step 1: Download image from GCS
        redis_service.set_analysis_progress(analysis_id, 10)
        with ANALYSIS_DURATION.labels(stage='download_image').time():
            image_bytes = _download_image(image_url)

        # Step 2: Detect objects with Vision API (ì™¸ë¶€ API í˜¸ì¶œ)
        redis_service.set_analysis_progress(analysis_id, 20)
        with ANALYSIS_DURATION.labels(stage='detect_objects').time():
            detected_items = _detect_objects(image_bytes)
        logger.info(f"Detected {len(detected_items)} items")

        if not detected_items:
            redis_service.update_analysis_done(analysis_id, {'items': []})
            _update_analysis_status_db(analysis_id, 'DONE')
            return {'analysis_id': analysis_id, 'items': []}

        # Step 3: ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•´ ì´ë¯¸ì§€ë¥¼ base64ë¡œ ì¸ì½”ë”©
        # (CeleryëŠ” bytesë¥¼ ì§ì ‘ ì „ë‹¬í•˜ê¸° ì–´ë ¤ì›€)
        image_b64 = base64.b64encode(image_bytes).decode('utf-8')

        # Step 4: ê° ê°ì²´ë³„ ì„œë¸ŒíƒœìŠ¤í¬ ìƒì„± (ë³‘ë ¬ ì²˜ë¦¬)
        subtasks = []
        for idx, item in enumerate(detected_items):
            subtasks.append(
                process_single_item.s(
                    analysis_id=analysis_id,
                    image_b64=image_b64,
                    detected_item_dict={
                        'category': item.category,
                        'bbox': {
                            'x_min': item.bbox.x_min,
                            'y_min': item.bbox.y_min,
                            'x_max': item.bbox.x_max,
                            'y_max': item.bbox.y_max,
                        },
                        'confidence': item.confidence,
                    },
                    item_index=idx,
                )
            )

        # Step 5: Celery chordë¡œ ë³‘ë ¬ ì‹¤í–‰ í›„ ê²°ê³¼ ìˆ˜ì§‘
        # chord: ëª¨ë“  ì„œë¸ŒíƒœìŠ¤í¬ ì™„ë£Œ í›„ ì½œë°± ì‹¤í–‰
        callback = analysis_complete_callback.s(
            analysis_id=analysis_id,
            user_id=user_id,
            total_items=len(detected_items),
        )

        job = chord(subtasks)(callback)
        logger.info(f"Analysis {analysis_id}: dispatched {len(subtasks)} parallel tasks")

        return {'analysis_id': analysis_id, 'status': 'PROCESSING', 'task_count': len(subtasks)}

    except Exception as e:
        logger.error(f"Analysis {analysis_id} failed: {e}")
        redis_service.update_analysis_failed(analysis_id, str(e))
        _update_analysis_status_db(analysis_id, 'FAILED')
        raise self.retry(exc=e)


@shared_task(bind=True, max_retries=2, default_retry_delay=30)
def process_single_item(
    self,
    analysis_id: str,
    image_b64: str,
    detected_item_dict: dict,
    item_index: int,
):
    """
    ë‹¨ì¼ ê²€ì¶œ ê°ì²´ ì²˜ë¦¬ íƒœìŠ¤í¬ (ë³‘ë ¬ ì‹¤í–‰ë¨).

    ì™¸ë¶€ API í˜¸ì¶œ:
    - Claude Vision: ì†ì„± ì¶”ì¶œ (color, brand, style)
    - FashionCLIP: ì´ë¯¸ì§€ ì„ë² ë”© ìƒì„±
    - OpenSearch: k-NN í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰

    Args:
        analysis_id: Analysis job ID
        image_b64: Base64 encoded image
        detected_item_dict: Detected item as dict
        item_index: Item index

    Returns:
        Processed item result
    """
    redis_service = get_redis_service()

    try:
        # Base64 ë””ì½”ë”©
        image_bytes = base64.b64decode(image_b64)

        # DetectedItem ë³µì›
        detected_item = DetectedItem(
            category=detected_item_dict['category'],
            bbox=type('BBox', (), detected_item_dict['bbox'])(),
            confidence=detected_item_dict['confidence'],
        )

        # ê°ì²´ ì²˜ë¦¬ (í¬ë¡­ â†’ ì„ë² ë”© â†’ ê²€ìƒ‰ â†’ ë¦¬ë­í‚¹)
        result = _process_detected_item(
            analysis_id=analysis_id,
            image_bytes=image_bytes,
            detected_item=detected_item,
            item_index=item_index,
        )

        # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
        completed_key = f"analysis:{analysis_id}:completed"
        current = redis_service.get(completed_key) or "0"
        redis_service.set(completed_key, str(int(current) + 1), ttl=3600)

        logger.info(f"Analysis {analysis_id} item {item_index} processed")
        return result

    except Exception as e:
        logger.error(f"Failed to process item {item_index} for analysis {analysis_id}: {e}")
        raise self.retry(exc=e)


@shared_task
def analysis_complete_callback(
    results: list[dict],
    analysis_id: str,
    user_id: Optional[int],
    total_items: int,
):
    """
    ëª¨ë“  ê°ì²´ ì²˜ë¦¬ ì™„ë£Œ í›„ í˜¸ì¶œë˜ëŠ” ì½œë°± íƒœìŠ¤í¬.
    ê²°ê³¼ë¥¼ DBì— ì €ì¥í•˜ê³  ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
    
    ğŸ†• ì¶”ê°€: ë¶„ì„ ì™„ë£Œ í›„ ê° ìƒí’ˆì— ëŒ€í•´ ìë™ í”¼íŒ… ìš”ì²­

    Args:
        results: ê° ì„œë¸ŒíƒœìŠ¤í¬ì˜ ê²°ê³¼ ëª©ë¡
        analysis_id: Analysis job ID
        user_id: Optional user ID
        total_items: Total detected items count

    Returns:
        Final analysis result
    """
    redis_service = get_redis_service()

    try:
        # None ê²°ê³¼ í•„í„°ë§
        valid_results = [r for r in results if r is not None]

        # DBì— ê²°ê³¼ ì €ì¥
        redis_service.set_analysis_progress(analysis_id, 90)
        with ANALYSIS_DURATION.labels(stage='save_results').time():
            _save_analysis_results(analysis_id, valid_results, user_id)

        # ì™„ë£Œ ìƒíƒœ ì—…ë°ì´íŠ¸
        redis_service.update_analysis_done(analysis_id, {'items': valid_results})
        _update_analysis_status_db(analysis_id, 'DONE')

        # Metrics: ë¶„ì„ ì™„ë£Œ
        ANALYSIS_TOTAL.labels(status='success').inc()
        ANALYSES_COMPLETED_TOTAL.inc()
        ANALYSIS_IN_PROGRESS.dec()

        # Metrics: ë§¤ì¹­ëœ ìƒí’ˆ ìˆ˜
        for result in valid_results:
            category = result.get('category', 'unknown')
            match_count = len(result.get('matches', []))
            for _ in range(match_count):
                PRODUCT_MATCHES_TOTAL.labels(category=category).inc()

        logger.info(f"Analysis {analysis_id} completed: {len(valid_results)}/{total_items} items processed")

        # ğŸ†• ë¶„ì„ ì™„ë£Œ í›„ ìë™ í”¼íŒ… ìš”ì²­
        if user_id:
            _trigger_auto_fittings(analysis_id, valid_results, user_id)

        # Celery ì›Œì»¤ ë©”íŠ¸ë¦­ì„ Pushgatewayë¡œ í‘¸ì‹œ
        push_metrics()

        return {
            'analysis_id': analysis_id,
            'status': 'DONE',
            'processed_items': len(valid_results),
            'total_items': total_items,
        }

    except Exception as e:
        logger.error(f"Failed to complete analysis {analysis_id}: {e}")
        redis_service.update_analysis_failed(analysis_id, str(e))
        _update_analysis_status_db(analysis_id, 'FAILED')

        # Metrics: ë¶„ì„ ì‹¤íŒ¨
        ANALYSIS_TOTAL.labels(status='failed').inc()
        ANALYSIS_IN_PROGRESS.dec()

        # ì‹¤íŒ¨ ì‹œì—ë„ ë©”íŠ¸ë¦­ í‘¸ì‹œ
        push_metrics()

        return {
            'analysis_id': analysis_id,
            'status': 'FAILED',
            'error': str(e),
        }


@shared_task
def process_detected_item_task(
    analysis_id: str,
    image_bytes: bytes,
    detected_item_dict: dict,
    item_index: int,
):
    """
    Task for processing a single detected item (for parallel processing).

    Args:
        analysis_id: Analysis job ID
        image_bytes: Original image bytes
        detected_item_dict: Detected item as dict
        item_index: Item index

    Returns:
        Processed item result
    """
    detected_item = DetectedItem(
        category=detected_item_dict['category'],
        bbox=detected_item_dict['bbox'],
        confidence=detected_item_dict['confidence'],
    )

    return _process_detected_item(
        analysis_id=analysis_id,
        image_bytes=image_bytes,
        detected_item=detected_item,
        item_index=item_index,
    )


# =============================================================================
# Helper Functions
# =============================================================================

def _update_analysis_status_db(analysis_id: str, status: str):
    """DBì˜ ImageAnalysis ìƒíƒœ ì—…ë°ì´íŠ¸ í—¬í¼ í•¨ìˆ˜."""
    from analyses.models import ImageAnalysis
    try:
        analysis = ImageAnalysis.objects.get(id=analysis_id)
        analysis.image_analysis_status = status
        analysis.save(update_fields=['image_analysis_status', 'updated_at'])
    except ImageAnalysis.DoesNotExist:
        logger.error(f"ImageAnalysis {analysis_id} not found for status update")


def _download_image(image_url: str) -> bytes:
    """Download image from URL or local file path."""
    import os
    from google.cloud import storage

    # Convert GCS HTTPS URL to gs:// format
    # https://storage.googleapis.com/bucket/path -> gs://bucket/path
    if 'storage.googleapis.com' in image_url:
        parts = image_url.split('storage.googleapis.com/')
        if len(parts) > 1:
            image_url = 'gs://' + parts[1]

    # Parse GCS URL: gs://bucket/path/to/file
    if image_url.startswith('gs://'):
        parts = image_url[5:].split('/', 1)
        bucket_name = parts[0]
        blob_name = parts[1] if len(parts) > 1 else ''

        client = storage.Client()
        bucket = client.bucket(bucket_name)
        blob = bucket.blob(blob_name)

        return blob.download_as_bytes()
    elif image_url.startswith('/media/'):
        # Local media file - read directly from filesystem
        local_path = settings.BASE_DIR / image_url.lstrip('/')
        if not os.path.exists(local_path):
            raise FileNotFoundError(f"Local file not found: {local_path}")
        with open(local_path, 'rb') as f:
            return f.read()
    elif image_url.startswith('http://') or image_url.startswith('https://'):
        # HTTP URL - use requests
        import requests
        response = requests.get(image_url, timeout=30)
        response.raise_for_status()
        return response.content
    else:
        raise ValueError(f"Unsupported URL format: {image_url}")


def _detect_objects(image_bytes: bytes) -> list[DetectedItem]:
    """Detect fashion items in image."""
    vision_service = get_vision_service()
    return vision_service.detect_objects_from_bytes(image_bytes)


def _process_detected_item(
    analysis_id: str,
    image_bytes: bytes,
    detected_item: DetectedItem,
    item_index: int,
) -> Optional[dict]:
    """
    Process a single detected item.

    1. Crop the item from image
    2. Upload to GCS
    3. Extract attributes with Claude Vision (color, brand, etc.)
    4. Generate embedding
    5. Search similar products (with attribute filtering)

    Args:
        analysis_id: Analysis job ID
        image_bytes: Original image bytes
        detected_item: Detected item
        item_index: Item index

    Returns:
        Processed item result
    """
    try:
        # Step 1: Crop image and get pixel bbox
        cropped_bytes, pixel_bbox = _crop_image(image_bytes, detected_item)

        # Step 2: Upload cropped image to GCS
        cropped_image_url = _upload_to_gcs(
            image_bytes=cropped_bytes,
            analysis_id=analysis_id,
            item_index=item_index,
            category=detected_item.category,
        )

        # Step 3: Extract attributes with GPT-4V
        from services.gpt4v_service import get_gpt4v_service
        try:
            gpt4v_service = get_gpt4v_service()
            with ANALYSIS_DURATION.labels(stage='extract_attributes').time():
                attributes = gpt4v_service.extract_attributes(
                    image_bytes=cropped_bytes,
                    category=detected_item.category,
                )
            logger.info(f"Item {item_index} - GPT-4V attributes: color={attributes.color}, secondary_color={attributes.secondary_color}, brand={attributes.brand}")
        except Exception as e:
            logger.warning(f"GPT-4V extraction failed: {e}")
            attributes = None

        # Step 4: Generate embedding
        embedding_service = get_embedding_service()
        with ANALYSIS_DURATION.labels(stage='generate_embedding').time():
            embedding = embedding_service.get_image_embedding(cropped_bytes)

        # Step 5: Search similar products
        # Vision API ì¹´í…Œê³ ë¦¬ â†’ OpenSearch ì¹´í…Œê³ ë¦¬ ë§¤í•‘
        category_mapping = {
            'bottom': 'pants',
            'outerwear': 'outer',
        }
        search_category = category_mapping.get(detected_item.category, detected_item.category)

        opensearch_service = OpenSearchService()

        # ë¸Œëœë“œ ë˜ëŠ” ìƒ‰ìƒì´ ê°ì§€ë˜ë©´ ì†ì„± ê¸°ë°˜ ê²€ìƒ‰, ì•„ë‹ˆë©´ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
        detected_brand = attributes.brand if attributes else None
        detected_color = attributes.color if attributes else None
        detected_secondary = attributes.secondary_color if attributes else None

        if detected_brand or detected_color:
            logger.info(f"Item {item_index} - Using attribute search (brand={detected_brand}, color={detected_color}, secondary={detected_secondary})")
            with ANALYSIS_DURATION.labels(stage='search_products').time():
                search_results = opensearch_service.search_with_attributes(
                    embedding=embedding,
                    category=search_category,
                    brand=detected_brand,
                    color=detected_color,
                    secondary_color=detected_secondary,
                    k=30,
                    search_k=400,
                )
        else:
            # ì†ì„± ì—†ìœ¼ë©´ ê¸°ì¡´ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
            logger.info(f"Item {item_index} - No attributes, using hybrid search")
            with ANALYSIS_DURATION.labels(stage='search_products').time():
                search_results = opensearch_service.search_similar_products_hybrid(
                    embedding=embedding,
                    category=search_category,
                    k=30,
                    search_k=400,
                )

        if not search_results:
            logger.warning(f"No matching products found for item {item_index}")
            return None

        # Claude ë¦¬ë­í‚¹ìœ¼ë¡œ ì •í™•ë„ í–¥ìƒ
        from services.gpt4v_service import get_gpt4v_service
        try:
            gpt4v_service = get_gpt4v_service()
            with ANALYSIS_DURATION.labels(stage='rerank_products').time():
                search_results = gpt4v_service.rerank_products(
                    query_image_bytes=cropped_bytes,
                    candidates=search_results[:10],  # ìƒìœ„ 10ê°œë§Œ ë¦¬ë­í‚¹
                    top_k=5,
                )
            logger.info(f"Item {item_index} - Claude reranking completed")
        except Exception as e:
            logger.warning(f"Claude reranking failed, using original results: {e}")
            search_results = search_results[:5]

        # ìƒìœ„ 5ê°œ ë§¤ì¹­ ê²°ê³¼ ë°˜í™˜
        top_matches = []
        for match in search_results[:5]:
            top_matches.append({
                'product_id': match['product_id'],
                'score': match.get('combined_score', match['score']),
                'name': match.get('name'),
                'image_url': match.get('image_url'),
                'price': match.get('price'),
            })

        # ê²°ê³¼ì— ì¶”ì¶œëœ ì†ì„± ì •ë³´ í¬í•¨
        result = {
            'index': item_index,
            'category': detected_item.category,
            'bbox': pixel_bbox,
            'confidence': detected_item.confidence,
            'cropped_image_url': cropped_image_url,  # GCS URL
            'matches': top_matches,  # ìƒìœ„ 5ê°œ
        }

        # GPT-4V ì†ì„± ì¶”ê°€ (ìˆìœ¼ë©´)
        if attributes:
            result['attributes'] = {
                'color': attributes.color,
                'secondary_color': attributes.secondary_color,
                'brand': attributes.brand,
                'material': attributes.material,
                'style': attributes.style,
                'pattern': attributes.pattern,
            }

        return result

    except Exception as e:
        logger.error(f"Failed to process item {item_index}: {e}")
        return None


def _crop_image(image_bytes: bytes, item: DetectedItem, padding_ratio: float = 0.25) -> tuple[bytes, dict]:
    """
    Crop detected item from image with padding for better embedding quality.

    Args:
        image_bytes: Original image bytes
        item: Detected item with bounding box
        padding_ratio: Padding as ratio of bbox size (default 25%)

    Returns:
        Tuple of (cropped image bytes, pixel bbox dict)
    """
    image = Image.open(io.BytesIO(image_bytes))
    width, height = image.size

    # Convert normalized coordinates (0-1000) to pixels
    bbox = item.bbox
    x_min = int(bbox.x_min * width / 1000)
    y_min = int(bbox.y_min * height / 1000)
    x_max = int(bbox.x_max * width / 1000)
    y_max = int(bbox.y_max * height / 1000)

    # Original pixel bbox for storage (without padding)
    pixel_bbox = {
        'x_min': x_min,
        'y_min': y_min,
        'x_max': x_max,
        'y_max': y_max,
        'width': x_max - x_min,
        'height': y_max - y_min,
    }

    # Add padding for better embedding (more context helps CLIP)
    bbox_width = x_max - x_min
    bbox_height = y_max - y_min
    pad_x = int(bbox_width * padding_ratio)
    pad_y = int(bbox_height * padding_ratio)

    # Expanded bbox with padding (clamped to image bounds)
    crop_x_min = max(0, x_min - pad_x)
    crop_y_min = max(0, y_min - pad_y)
    crop_x_max = min(width, x_max + pad_x)
    crop_y_max = min(height, y_max + pad_y)

    # Crop with padding
    cropped = image.crop((crop_x_min, crop_y_min, crop_x_max, crop_y_max))

    # Convert to bytes
    output = io.BytesIO()
    cropped.save(output, format='JPEG', quality=95)
    return output.getvalue(), pixel_bbox


def _save_analysis_results(
    analysis_id: str,
    results: list[dict],
    user_id: Optional[int],
):
    """
    Save analysis results to MySQL.

    Updates:
    - ImageAnalysis: status = DONE
    - DetectedObject: insert detected items with bbox
    - ObjectProductMapping: insert top matches for each object
    """
    from analyses.models import ImageAnalysis, DetectedObject, ObjectProductMapping
    from products.models import Product

    try:
        # 1. ImageAnalysis ì¡°íšŒ ë° ìƒíƒœ ì—…ë°ì´íŠ¸
        analysis = ImageAnalysis.objects.select_related('uploaded_image').get(id=analysis_id)
        uploaded_image = analysis.uploaded_image

        # ì´ë¯¸ì§€ í¬ê¸° ê°€ì ¸ì˜¤ê¸° (ì •ê·œí™”ìš©)
        try:
            img = Image.open(uploaded_image.uploaded_image_url.path)
            img_width, img_height = img.size
        except Exception as e:
            logger.warning(f"Could not get image size: {e}, using default 1000x1000")
            img_width, img_height = 1000, 1000

        # 2. ê° ê²€ì¶œ ê²°ê³¼ì— ëŒ€í•´ DetectedObject ë° ë§¤í•‘ ìƒì„±
        for result in results:
            # bboxë¥¼ 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”
            bbox = result.get('bbox', {})
            normalized_bbox = {
                'x1': bbox.get('x_min', 0) / img_width,
                'y1': bbox.get('y_min', 0) / img_height,
                'x2': bbox.get('x_max', 0) / img_width,
                'y2': bbox.get('y_max', 0) / img_height,
            }

            # DetectedObject ìƒì„±
            detected_object = DetectedObject.objects.create(
                uploaded_image=uploaded_image,
                object_category=result.get('category', 'unknown'),
                bbox_x1=normalized_bbox['x1'],
                bbox_y1=normalized_bbox['y1'],
                bbox_x2=normalized_bbox['x2'],
                bbox_y2=normalized_bbox['y2'],
            )

            logger.info(f"Created DetectedObject {detected_object.id} - {result.get('category')}")

            # ObjectProductMapping ìƒì„± (ìƒìœ„ ë§¤ì¹­ ìƒí’ˆë“¤ - Product ìë™ ìƒì„± í¬í•¨)
            matches = result.get('matches', [])
            mapping_count = 0
            for match in matches:
                product_id = match.get('product_id')  # ë¬´ì‹ ì‚¬ itemId
                if product_id:
                    try:
                        # 1. ê¸°ì¡´ Product ê²€ìƒ‰
                        product = Product.objects.filter(
                            product_url__endswith=f'/{product_id}'
                        ).first()

                        # 2. ì—†ìœ¼ë©´ ê²€ìƒ‰ ê²°ê³¼ë¡œ ìë™ ìƒì„±
                        if not product:
                            product, created = Product.objects.update_or_create(
                                product_url=f'https://www.musinsa.com/app/goods/{product_id}',
                                defaults={
                                    'brand_name': match.get('brand', 'Unknown') or 'Unknown',
                                    'product_name': match.get('name', 'Unknown') or 'Unknown',
                                    'category': result.get('category', 'unknown'),
                                    'selling_price': int(match.get('price', 0) or 0),
                                    'product_image_url': match.get('image_url', '') or '',
                                }
                            )
                            if created:
                                logger.info(f"Auto-created Product {product_id}")

                        # 3. ë§¤í•‘ ìƒì„±
                        ObjectProductMapping.objects.create(
                            detected_object=detected_object,
                            product=product,
                            confidence_score=match.get('score', 0.0),
                        )
                        mapping_count += 1

                    except Exception as e:
                        logger.warning(f"Error creating mapping for product {product_id}: {e}")

            logger.info(f"Created {mapping_count} mappings for object {detected_object.id}")

        # 3. ImageAnalysis ìƒíƒœ ì—…ë°ì´íŠ¸
        analysis.image_analysis_status = ImageAnalysis.Status.DONE
        analysis.save()

        logger.info(f"Successfully saved {len(results)} results for analysis {analysis_id}")

    except ImageAnalysis.DoesNotExist:
        logger.error(f"ImageAnalysis {analysis_id} not found")
    except Exception as e:
        logger.error(f"Failed to save analysis results: {e}")


def _trigger_auto_fittings(analysis_id: str, results: list[dict], user_id: int):
    """
    ë¶„ì„ ì™„ë£Œ í›„ ìë™ í”¼íŒ… ìš”ì²­ íŠ¸ë¦¬ê±°.
    
    ì‚¬ìš©ìì˜ ì „ì‹  ì´ë¯¸ì§€ì™€ ê° ë§¤ì¹­ëœ ìƒí’ˆì— ëŒ€í•´
    ìë™ìœ¼ë¡œ í”¼íŒ… íƒœìŠ¤í¬ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
    
    Args:
        analysis_id: Analysis job ID
        results: ë¶„ì„ ê²°ê³¼ (ê° ê²€ì¶œ ê°ì²´ì™€ ë§¤ì¹­ëœ ìƒí’ˆë“¤)
        user_id: ì‚¬ìš©ì ID
    """
    from fittings.models import UserImage, FittingImage
    from fittings.tasks import process_fitting_task
    from products.models import Product
    
    try:
        # 1. ì‚¬ìš©ìì˜ ì „ì‹  ì´ë¯¸ì§€ ì¡°íšŒ (ê°€ì¥ ìµœê·¼ ê²ƒ)
        user_image = UserImage.objects.filter(
            user_id=user_id,
            is_deleted=False
        ).order_by('-created_at').first()
        
        if not user_image:
            logger.warning(f"User {user_id} has no body image, skipping auto-fitting")
            return
        
        logger.info(f"Auto-fitting: Found user image {user_image.id} for user {user_id}")
        
        # 2. ê° ê²€ì¶œ ê°ì²´ì˜ ìƒìœ„ ë§¤ì¹­ ìƒí’ˆì— ëŒ€í•´ í”¼íŒ… ìš”ì²­
        fitting_count = 0
        for result in results:
            matches = result.get('matches', [])
            if not matches:
                continue
            
            # ìƒìœ„ 1ê°œ ìƒí’ˆì— ëŒ€í•´ì„œë§Œ í”¼íŒ… (í•„ìš”ì‹œ ì¡°ì • ê°€ëŠ¥)
            top_match = matches[0]
            product_id = top_match.get('product_id')
            
            if not product_id:
                continue
            
            try:
                # Product ì¡°íšŒ
                product = Product.objects.filter(
                    product_url__endswith=f'/{product_id}'
                ).first()
                
                if not product:
                    logger.warning(f"Product {product_id} not found, skipping")
                    continue
                
                # ì´ë¯¸ í”¼íŒ…ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸ (ì¤‘ë³µ ë°©ì§€)
                existing_fitting = FittingImage.objects.filter(
                    user_image=user_image,
                    product=product,
                    is_deleted=False
                ).first()
                
                if existing_fitting:
                    logger.info(f"Fitting already exists for product {product.id}, skipping")
                    continue
                
                # FittingImage ìƒì„± (PENDING ìƒíƒœ)
                fitting = FittingImage.objects.create(
                    user_image=user_image,
                    product=product,
                    fitting_image_status=FittingImage.Status.PENDING,
                )
                
                # í”¼íŒ… íƒœìŠ¤í¬ ë¹„ë™ê¸° ì‹¤í–‰
                process_fitting_task.delay(fitting.id)
                fitting_count += 1
                
                logger.info(f"Auto-fitting triggered: FittingImage {fitting.id} for product {product.id}")
                
            except Exception as e:
                logger.error(f"Failed to trigger fitting for product {product_id}: {e}")
        
        logger.info(f"Auto-fitting completed: {fitting_count} fittings triggered for analysis {analysis_id}")
        
    except Exception as e:
        logger.error(f"Failed to trigger auto-fittings for analysis {analysis_id}: {e}")

